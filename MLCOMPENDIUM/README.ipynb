{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Compendium\n",
    "\n",
    "![randomimage](https://velocityglobal.com/wp-content/uploads/2019/02/Blog-Images-Forget-Machine-Learning-Humans-Still-Have-a-Lot-to-Learn-Part-II.jpg)\n",
    "\n",
    "This is a compendium of all my learning from Kaggle distilled into one page. Please don't use this as a ref only and not a primary learning source, please go direct to kaggle.com at the learn section if you wish to gain the full learning experience.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONTENTS  \n",
    "  \n",
    "  \n",
    "## Basics \n",
    "[1] - [Terminology](#Terminology)  \n",
    "[2] - [High level steps](#STEPS)  \n",
    "[3] - [Mean Absolute Error](#MEAN-ABSOLUTE-ERROR)  \n",
    "[4] - [Examples](#EXAMPLES)  \n",
    "[5] - [Full Code Sample](#FULL-CODE-SAMPLE) \n",
    "\n",
    "## Techniques  \n",
    "[1] - [Work out data types](#Get-list-of-categorical-variables)  \n",
    "[2] - [Managing Missing Values](#Managing-Missing-Values)  \n",
    "[3] - [Managing Catagorical Variables](#CATAGORICAL-VARIABLES)  \n",
    "[4] - [Onehot Encoding](#One-Hot-Encoding)  \n",
    "[5] - [Catagorical Processing code](#Catagorical-Processing-Functions)\n",
    "\n",
    "## Model Types \n",
    "  \n",
    "[1] - [Decision tree](#DECISION-TREE)  \n",
    "[2] - [Random Forrest](#RANDOM-FORREST)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEPS \n",
    "The steps to building and using a model are:\n",
    "\n",
    "**Define:** What type of model will it be? A decision tree? Some other type of model? Some other parameters of the model type are specified too.  \n",
    "**Fit:** Capture patterns from provided data. This is the heart of modeling.  \n",
    "**Predict:** Just what it sounds like  \n",
    "**Evaluate:** Determine how accurate the model's predictions are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EXAMPLES\n",
    "#### [main Menu](#CONTENTS)    \n",
    "\n",
    "## SIMPLE FITTING \n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Define model. Specify a number for random_state to ensure same results each run\n",
    "MYMODEL = DecisionTreeRegressor(random_state=1)\n",
    "\n",
    "# Fit model\n",
    "MYMODEL.fit(X, y)\n",
    "```\n",
    "\n",
    "## MEAN ABSOLUTE ERROR \n",
    "  \n",
    "Information on MAE can be found in the terminology section at the bottom.  \n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "predicted_home_prices = melbourne_model.predict(X)\n",
    "\n",
    "mean_absolute_error(y, predicted_home_prices) \n",
    "\n",
    "```  \n",
    "  \n",
    "## SPLITTING DATA \n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# split data into training and validation data, for both features and target\n",
    "# The split is based on a random number generator. Supplying a numeric value to\n",
    "# the random_state argument guarantees we get the same split every time we\n",
    "# run this script.\n",
    "# SPLIT DATA \n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state = 0)\n",
    "\n",
    "#-----------------------------------------------  Define model\n",
    "melbourne_model = DecisionTreeRegressor()\n",
    "\n",
    "#----------------------------------------------- Fit model\n",
    "melbourne_model.fit(train_X, train_y)\n",
    "\n",
    "#-----------------------------------------------  get predictions on VALIDATION data \n",
    "val_predictions = melbourne_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, val_predictions))\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL CODE SAMPLE \n",
    "## DecisionTreeRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Path of the file to read\n",
    "iowa_file_path = '../input/home-data-for-ml-course/train.csv'\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "\n",
    "# PRINT COLUMNS\n",
    "home_data.columns\n",
    "\n",
    "# TARGET\n",
    "y = home_data.SalePrice\n",
    "\n",
    "# SLICE TO COLUMNS\n",
    "feature_names = ['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr']\n",
    "\n",
    "# SELECT COLUMNS FEATURES\n",
    "X = home_data[feature_names]\n",
    "\n",
    "# FIT MODEL\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "model = DecisionTreeRegressor(random_state=1)\n",
    "model.fit(X,y)\n",
    "\n",
    "# PREDICT\n",
    "\n",
    "prediction = model.predict(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DECISION TREE   \n",
    "#### [main Menu](#CONTENTS)\n",
    "  \n",
    "    \n",
    "- LOTS OF OPTIONS \n",
    "- LEAF = ONE UNIT\n",
    "- DEPTH = SPLIT\n",
    "\n",
    "Depth of 10 means 2^10 (1024) leaves. \n",
    "  \n",
    "- max_leaf_nodes helps us limmit overfitting \n",
    "  \n",
    "## Important     \n",
    "### MAE UTILITY FUNCTION \n",
    "\n",
    "We can use a utility function to help compare **MAE scores** from different values for max_leaf_nodes:   \n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "\n",
    "\n",
    "# compare MAE with differing values of max_leaf_nodes\t\n",
    "\n",
    "for max_leaf_nodes in [5, 50, 500, 5000]:\n",
    "    my_mae = get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y)\n",
    "    print(\"Max leaf nodes: %d  \\t\\t Mean Absolute Error:  %d\" %(max_leaf_nodes, my_mae))\n",
    "    \n",
    "```\n",
    "  \n",
    "    \n",
    "```shell\n",
    "Max leaf nodes: 5  \t\t Mean Absolute Error:  347380\n",
    "Max leaf nodes: 50  \t\t Mean Absolute Error:  258171\n",
    "Max leaf nodes: 500  \t\t Mean Absolute Error:  243495\n",
    "Max leaf nodes: 5000  \t Mean Absolute Error:  254983\n",
    "```\n",
    "  \n",
    "    \n",
    "```python \n",
    "# OBTAIN BEST HYPER PARAMETER USING LAMBDA AND DICT \n",
    "scores = {leaf_size: get_mae(leaf_size, train_X, val_X, train_y, val_y) for leaf_size in candidate_max_leaf_nodes}\n",
    "best_tree_size = min(scores, key=scores.get)\n",
    "```\n",
    "\n",
    "## FULL DECISION TREE CODE \n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error \n",
    "\n",
    "#PREP\n",
    "iowa_file_path = 'train.csv'\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "home_data.columns\n",
    "y = home_data.SalePrice\n",
    "feature_names = ['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr']\n",
    "X = home_data[feature_names]\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# OPTIMISE\n",
    "def get_mae(max_leaf_nodes, train_X, val_X, train_y, val_y):\n",
    "    model = DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes, random_state=0)\n",
    "    model.fit(train_X, train_y)\n",
    "    preds_val = model.predict(val_X)\n",
    "    mae = mean_absolute_error(val_y, preds_val)\n",
    "    return(mae)\n",
    "\n",
    "# ITERATE\n",
    "candidate_max_leaf_nodes = [5, 25, 50, 100, 250, 500]\n",
    "leaf_dict = {}\n",
    "for leaf_size in candidate_max_leaf_nodes:\n",
    "    current_min = get_mae(leaf_size, train_X, val_X, train_y, val_y)\n",
    "    leaf_dict[leaf_size] = current_min\n",
    "\n",
    "best_tree_size = min(leaf_dict, key=leaf_dict.get) \n",
    "\n",
    "# Final result using `best_tree_size`. \n",
    "\n",
    "final_model = DecisionTreeRegressor(max_leaf_nodes=best_tree_size, random_state=1)\n",
    "final_model.fit(X, y) # FIT ON ALL DATA, NOT JUST TRAIN\n",
    "prediction = final_model.predict(val_X)\n",
    "val_mae = mean_absolute_error(val_y, prediction)\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forrest  \n",
    "#### [main Menu](#CONTENTS)  \n",
    "  \n",
    "- MUCH BETTER THAN DECISION TREE\n",
    "- EVEN WITHOUT TUNING IT'S GOOD \n",
    "\n",
    "## EXAMPLE \n",
    "  \n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "forest_model = RandomForestRegressor(random_state=1)\n",
    "forest_model.fit(train_X, train_y)\n",
    "melb_preds = forest_model.predict(val_X)\n",
    "print(mean_absolute_error(val_y, melb_preds))\n",
    "```\n",
    "\n",
    "\n",
    "## Full Code with Optimiser  \n",
    "\n",
    "```python \n",
    "\n",
    "# Code you have previously used to load data\n",
    "import pandas as pd\n",
    "# FIT MODEL\n",
    "#from sklearn.tree import DecisionTreeRegressor NOT NEEDED NOW\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Import the train_test_split function and uncomment\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "# STANDARD PREP APPROACH\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "# Path of the file to read\n",
    "iowa_file_path = 'train.csv'\n",
    "home_data = pd.read_csv(iowa_file_path)\n",
    "# print the list of columns (to find the target we want to predict) \n",
    "home_data.columns\n",
    "# set target output\n",
    "y = home_data.SalePrice\n",
    "# SLICE THE HOME DATA INTO TARGETTED COLUMNS ONLY\n",
    "features = ['LotArea','YearBuilt','1stFlrSF','2ndFlrSF','FullBath','BedroomAbvGr']\n",
    "# select data corresponding to features in features\n",
    "X = home_data[features]\n",
    "# Split into validation and training data\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "# OPTIMISER\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "model_1 = RandomForestRegressor(n_estimators = 50, random_state = 0)\n",
    "model_2 = RandomForestRegressor(n_estimators = 100, random_state = 0)\n",
    "model_3 = RandomForestRegressor(n_estimators = 100, random_state = 0, criterion = 'mae')\n",
    "model_4 = RandomForestRegressor(n_estimators = 200, random_state = 0, min_samples_split = 20)\n",
    "model_5 = RandomForestRegressor(n_estimators = 100, random_state = 0, max_depth = 7)\n",
    "model_6 = RandomForestRegressor(n_estimators = 100, random_state = 1)\n",
    "\n",
    "\n",
    "models = [model_1,model_2,model_3,model_4,model_5,model_6]\n",
    "\n",
    "# COMPARE DIFFERENT MODELS FUNCTION\n",
    "\n",
    "def score_model(model, X_t = train_X, X_v = val_X, y_t = train_y, y_v= val_y):\n",
    "    model.fit(X_t, y_t)\n",
    "    preds = model.predict(X_v)\n",
    "    return mean_absolute_error(y_v, preds)  \n",
    "\n",
    "\n",
    "mae = 999999 # make it high \n",
    "mae_index = 0\n",
    "\n",
    "for i in range(0, len(models)):\n",
    "\tmae_current = score_model(models[i])\n",
    "\tif mae_current < mae:\n",
    "\t\tmae = mae_current\n",
    "\t\tmae_index = i\n",
    "\tprint(\"Model %d MAE: %d\" % (i+1, mae_current))\n",
    "\n",
    "print(\"Min MAE value is : \" + str(mae) + ' and the model index is ' + str(mae_index))\n",
    "#------------------------------------------------------------------------------------------\n",
    "# BUILD  MODEL\n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "model = models[mae_index]\n",
    "model.fit(train_X, train_y)\n",
    "\n",
    "prediction = model.predict(val_X)\n",
    "MAE = mean_absolute_error(prediction, val_y)\n",
    "\n",
    "\n",
    "#------------------------------------------------------------------------------------------\n",
    "# PRINT \n",
    "#------------------------------------------------------------------------------------------\n",
    "\n",
    "print(\"Validation MAE for Random Forest Model: {}\".format(MAE))\n",
    "\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Managing Missing Values  \n",
    "#### [main Menu](#CONTENTS)  \n",
    "## Three Approaches   \n",
    "  \n",
    "### DROP COLUMN \n",
    "  \n",
    "\n",
    "![](drop1.png)\n",
    "  \n",
    "**Easiest appraoch** only do this if most values are missing from column or you lose valuable data.  \n",
    "\n",
    "## Imputation  \n",
    "\n",
    "![](drop2.png). \n",
    "  \n",
    "**BETTER OPTION**  This fills in the missing value with a number (like the mean/average). \n",
    "  \n",
    "## Advanced Imputation \n",
    "  \n",
    "![](drop3.png)  \n",
    "  \n",
    "There may be a hidden pattern in the data that's missing *(maybe they are all above a certain threshold)* By adding a flag to notify which values were missing allows for better compute.  \n",
    "\n",
    "### Note Imputation can be worse because:  \n",
    "\n",
    "1. Noisy Data Set \n",
    "2. Maybe 0 means they don't have it in stock, \n",
    "3. Most frequent is more useful than Mean \n",
    "\n",
    "## MISSING VALUE COMPARE FUNCTIONS    \n",
    "#### [main Menu](#CONTENTS)    \n",
    "\n",
    "Define Function to Measure Quality of Each Approach \n",
    "### DROP\n",
    "####  Be  careful to drop the same columns in both DataFrames.\n",
    "\n",
    "```python \n",
    "\n",
    "# Get names of columns with missing values\n",
    "cols_with_missing = [col for col in X_train.columns\n",
    "                     if X_train[col].isnull().any()]\n",
    "\n",
    "# Drop columns in training and validation data\n",
    "reduced_X_train = X_train.drop(cols_with_missing, axis=1)\n",
    "reduced_X_valid = X_valid.drop(cols_with_missing, axis=1)\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop columns with missing values):\")\n",
    "print(score_dataset(reduced_X_train, reduced_X_valid, y_train, y_valid))\n",
    "\n",
    "```  \n",
    "  \n",
    "```shell\n",
    "MAE from Approach 1 (Drop columns with missing values):\n",
    "183550.22137772635\n",
    "```  \n",
    "  \n",
    "### IMPUTATION  \n",
    "  \n",
    "Next, we use SimpleImputer to replace missing values with the **mean value** along each column.  \n",
    "    Statisticians have experimented with various methods to deterimine imputed values but rarely offers more benefits.  \n",
    "#### NOTE: YOU NEED TO PUT COLUMNS BACK AFTER my_imputer.transform\n",
    "  \n",
    "```python \n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\n",
    "imputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train.columns = X_train.columns\n",
    "imputed_X_valid.columns = X_valid.columns\n",
    "\n",
    "print(\"MAE from Approach 2 (Imputation):\")\n",
    "print(score_dataset(imputed_X_train, imputed_X_valid, y_train, y_valid))\n",
    "\n",
    "```shell\n",
    "MAE from Approach 2 (Imputation):\n",
    "178166.46269899711\n",
    "```\n",
    "\n",
    "**LOWER MAE!!** (By about 7%)\n",
    "  \n",
    "### ADVANCED IMPUTATION  \n",
    "  \n",
    "Next, we impute the missing values, while also keeping track of which values were imputed.  \n",
    " \n",
    "```python\n",
    "# Make copy to avoid changing original data (when imputing)\n",
    "X_train_plus = X_train.copy()\n",
    "X_valid_plus = X_valid.copy()\n",
    "\n",
    "# Make new columns indicating what will be imputed\n",
    "for col in cols_with_missing:\n",
    "    X_train_plus[col + '_was_missing'] = X_train_plus[col].isnull()\n",
    "    X_valid_plus[col + '_was_missing'] = X_valid_plus[col].isnull()\n",
    "\n",
    "# Imputation\n",
    "my_imputer = SimpleImputer()\n",
    "imputed_X_train_plus = pd.DataFrame(my_imputer.fit_transform(X_train_plus))\n",
    "imputed_X_valid_plus = pd.DataFrame(my_imputer.transform(X_valid_plus))\n",
    "\n",
    "# Imputation removed column names; put them back\n",
    "imputed_X_train_plus.columns = X_train_plus.columns\n",
    "imputed_X_valid_plus.columns = X_valid_plus.columns\n",
    "\n",
    "print(\"MAE from Approach 3 (An Extension to Imputation):\")\n",
    "print(score_dataset(imputed_X_train_plus, imputed_X_valid_plus, y_train, y_valid))\n",
    "\n",
    "```\n",
    "  \n",
    "    \n",
    "```shell\n",
    "MAE from Approach 3 (An Extension to Imputation):\n",
    "178927.503183954\n",
    "```\n",
    "\n",
    "**worse** \n",
    "Challenge - find out why?  \n",
    "\n",
    "## Print sum of missing values \n",
    "#### [main Menu](#CONTENTS)   \n",
    "```python \n",
    "# Number of missing values in each column of training data\n",
    "missing_val_count_by_column = (X_train.isnull().sum())\n",
    "print(missing_val_count_by_column[missing_val_count_by_column > 0])\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CATAGORICAL VARIABLES  \n",
    "  \n",
    "#### [main Menu](#CONTENTS)     \n",
    "  \n",
    "A categorical variable takes only a limited number of values.  \n",
    " like `\"Honda\"`, `\"Toyota\"`, and `“Ford\"`. \n",
    " \n",
    "## Three Approaches   \n",
    "\n",
    "### Drop Catagorical Variables   \n",
    "\n",
    "Dropping the column works best if that column didn't have useful data.   \n",
    "\n",
    "### Label Encoding  \n",
    "  \n",
    "Each unique term `ford`, `toyota` gets a unique integer assigned `0`, `1` etc respectively. \n",
    "  \n",
    "![](label.png)  \n",
    "  \n",
    "    \n",
    "      \n",
    "This assumes the catagories have an order i.e. `never 0` < `rarely 1` < `most days 2` etc.  \n",
    "we refer to these  as **Ordinal Variables**   \n",
    "These work really well in Tree models (forrest etc).  \n",
    "\n",
    "Note not all catagories have a ranking and do not appear more or less than each other - **nominal variables**   \n",
    "In this case you can use **one-hot-encoding**\n",
    "\n",
    "\n",
    "## One-Hot-Encoding   \n",
    "  \n",
    "For every **unique** catagory, a **column** is added - the column then has a 0 - this row doesn't have it. 1 - this row has it.  \n",
    "  \n",
    "![](onehot.png)  \n",
    "  \n",
    "    \n",
    "- Don't use if more than 15 different vars\n",
    "- This works well when no clear order is assumed. \n",
    "  \n",
    "  \n",
    "## Get list of categorical variables\n",
    "  \n",
    "```python\n",
    "s = (X_train.dtypes == 'object')\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)\n",
    "```\n",
    "  \n",
    "```\n",
    "Categorical variables:\n",
    "['Type', 'Method', ‘Regionname'] \n",
    "  \n",
    "```\n",
    "\n",
    "\n",
    "## Catagorical Processing Functions \n",
    "#### [main Menu](#CONTENTS)      \n",
    "\n",
    "### Drop Categorical Variables\n",
    "  \n",
    "We drop the object columns with the select_dtypes() method  \n",
    "  \n",
    "```python \n",
    "\n",
    "\n",
    "drop_X_train = X_train.select_dtypes(exclude=['object'])\n",
    "drop_X_valid = X_valid.select_dtypes(exclude=['object'])\n",
    "\n",
    "print(\"MAE from Approach 1 (Drop categorical variables):\")\n",
    "print(score_dataset(drop_X_train, drop_X_valid, y_train, y_valid))\n",
    "\n",
    "```\n",
    "  \n",
    "    \n",
    "```\n",
    "MAE from Approach 1 (Drop categorical variables):\n",
    "175703.48185157913\n",
    "```\n",
    "\n",
    "\n",
    "## Label Encoding  \n",
    "  \n",
    "Scikit-learn has a LabelEncoder class that can be used to get label encodings. We loop over the categorical variables and apply the label encoder separately to each column.  \n",
    "\n",
    "```python\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Make copy to avoid changing original data \n",
    "label_X_train = X_train.copy()\n",
    "label_X_valid = X_valid.copy()\n",
    "\n",
    "# Apply label encoder to each column with categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "for col in object_cols:\n",
    "    label_X_train[col] = label_encoder.fit_transform(X_train[col])\n",
    "    label_X_valid[col] = label_encoder.transform(X_valid[col])\n",
    "\n",
    "print(\"MAE from Approach 2 (Label Encoding):\") \n",
    "print(score_dataset(label_X_train, label_X_valid, y_train, y_valid))\n",
    "\n",
    "```  \n",
    "```\n",
    "MAE from Approach 2 (Label Encoding):\n",
    "165936.40548390493  \n",
    "``` \n",
    "  \n",
    "## PROBLEMS WITH LABEL ENCODING\n",
    "  \n",
    "Take the following example:  \n",
    "```python \n",
    "Unique values in 'Condition2' column in training data: \n",
    "['Norm' 'PosA' 'Feedr' 'PosN' 'Artery' 'RRAe']\n",
    "\n",
    "Unique values in 'Condition2' column in validation data: \n",
    "['Norm' 'RRAn' 'RRNn' 'Artery' 'Feedr' 'PosN']\n",
    "```  \n",
    "\n",
    "1. Values are different (order)\n",
    "2. Converted to numbers they wont match up \n",
    "  \n",
    "### KEEP TRACK OF VIOLATING COLUMNS  \n",
    "\n",
    "```python\n",
    "# GET All categorical columns\n",
    "object_cols = [col for col in X_train.columns if X_train[col].dtype == \"object\"]\n",
    "\n",
    "# Columns that can be safely label encoded\n",
    "good_label_cols = [col for col in object_cols if set(X_train[col]) == set(X_valid[col])]\n",
    "        \n",
    "# Problematic columns that will be dropped from the dataset\n",
    "bad_label_cols = list(set(object_cols)-set(good_label_cols))\n",
    "        \n",
    "print('Categorical columns that will be label encoded:', good_label_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', bad_label_cols)\n",
    "```\n",
    "## One-Hot-Encoding   \n",
    "  \n",
    "   \n",
    "- We set `handle_unknown='ignore'` to avoid errors when the validation data contains classes that aren't represented in the training data, and\n",
    "- setting `sparse=False` ensures that the encoded columns are returned as a numpy array (instead of a sparse matrix).\n",
    "      \n",
    "To use the encoder, we supply only the categorical columns that we want to be one-hot encoded. For instance, to encode the training data, we supply `X_train[object_cols]`. (object_cols in the code cell below is a list of the column names with categorical data, and so `X_train[object_cols]` contains all of the categorical data in the training set.)  \n",
    "  \n",
    "  \n",
    "```python\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Apply one-hot encoder to each column with categorical data\n",
    "OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n",
    "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\n",
    "OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\n",
    "\n",
    "# One-hot encoding removed index; put it back\n",
    "OH_cols_train.index = X_train.index\n",
    "OH_cols_valid.index = X_valid.index\n",
    "\n",
    "# Remove categorical columns (will replace with one-hot encoding)\n",
    "num_X_train = X_train.drop(object_cols, axis=1)\n",
    "num_X_valid = X_valid.drop(object_cols, axis=1)\n",
    "\n",
    "# Add one-hot encoded columns to numerical features\n",
    "OH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\n",
    "OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n",
    "\n",
    "print(\"MAE from Approach 3 (One-Hot Encoding):\") \n",
    "print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))\n",
    "```\n",
    "  \n",
    "    \n",
    "```\n",
    "MAE from Approach 3 (One-Hot Encoding):\n",
    "166089.4893009678\n",
    "```  \n",
    "## Investigating cardinality (for one hot encoding)   \n",
    "  \n",
    "Consider the following:  \n",
    "  \n",
    "```python\n",
    "# Get number of unique entries in each column with categorical data\n",
    "object_nunique = list(map(lambda col: X_train[col].nunique(), object_cols))\n",
    "d = dict(zip(object_cols, object_nunique))\n",
    "\n",
    "# Print number of unique entries by column, in ascending order\n",
    "sorted(d.items(), key=lambda x: x[1])\n",
    "```\n",
    "```\n",
    "[('Street', 2),\n",
    " ('Utilities', 2),\n",
    " ('CentralAir', 2),\n",
    " ('LandSlope', 3),\n",
    " ('PavedDrive', 3),\n",
    " ('LotShape', 4),\n",
    " ('LandContour', 4),\n",
    " ('ExterQual', 4),\n",
    " ('KitchenQual', 4),\n",
    " ('MSZoning', 5),\n",
    " ('LotConfig', 5),\n",
    " ('BldgType', 5),\n",
    " ('ExterCond', 5),\n",
    " ('HeatingQC', 5),\n",
    " ('Condition2', 6),\n",
    " ('RoofStyle', 6),\n",
    " ('Foundation', 6),\n",
    " ('Heating', 6),\n",
    " ('Functional', 6),\n",
    " ('SaleCondition', 6),\n",
    " ('RoofMatl', 7),\n",
    " ('HouseStyle', 8),\n",
    " ('Condition1', 9),\n",
    " ('SaleType', 9),\n",
    " ('Exterior1st', 15),\n",
    " ('Exterior2nd', 16),\n",
    " ('Neighborhood', 25)]\n",
    " ```\n",
    "  \n",
    "We refer to the number of unique entries of a categorical variable as the **cardinality** of that categorical variable.  For instance, the `'Street'` variable has cardinality 2.  \n",
    "  \n",
    "#### The number of columns needed for one hot encoding = cardinality\n",
    "  \n",
    "- ONE HOT ENCODING CAN GREATLY EXPAND DATA SET\n",
    "- BEST TO ONE HOT ENCODE LOW CARDINALITY \n",
    "- LABEL ENCODE HIGH CARDINALITY \n",
    "\n",
    "Do it by creating a list for each\" \n",
    "```python\n",
    "  \n",
    "# Columns that will be one-hot encoded\n",
    "low_cardinality_cols = [col for col in object_cols if X_train[col].nunique() < 10]\n",
    "\n",
    "# Columns that will be dropped from the dataset\n",
    "high_cardinality_cols = list(set(object_cols)-set(low_cardinality_cols))\n",
    "\n",
    "print('Categorical columns that will be one-hot encoded:', low_cardinality_cols)\n",
    "print('\\nCategorical columns that will be dropped from the dataset:', high_cardinality_cols)\n",
    "```.  \n",
    "    \n",
    "**Best-Performing** In general, one-hot encoding (Approach 3) will typically perform best, and dropping the categorical columns (Approach 1) typically performs worst, but it varies on a case-by-case basis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipelines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Terminology\n",
    "#### [main Menu](#CONTENTS)  \n",
    "\n",
    "## Mean Absolute Error (MAE)\n",
    "MAE measures the average magnitude of the errors in a set of predictions, without considering their **direction**\n",
    "\n",
    "## Root mean squared error (RMSE)\n",
    "RMSE is a quadratic scoring rule that also measures the average magnitude of the error. It’s the square root of the average of squared differences between prediction and actual observation\n",
    "\n",
    "## Overfitting\n",
    "\n",
    "- ACCURATE FOR EXISTING DATA\n",
    "- POOR FOR NEW DATA\n",
    "\n",
    "## Underfitting\n",
    "\n",
    "- FAILS TO GENERALIZE FOR EXISTING DATA SET    \n",
    "\n",
    "  \n",
    "  \n",
    "  \n",
    "![graph](graph.png)  \n",
    "  \n",
    "  \n",
    "The middle (warning sign) is optimal location)   \n",
    "  \n",
    "## Ordinal Variables   \n",
    "  \n",
    "Catagorical variabels `ford` `toyota` etc that have a ranking (frequency ranking).   \n",
    "These work best in decision tree and random forrest models.    \n",
    "\n",
    "## Nominal Variables \n",
    "  \n",
    "The opposite of Ordinal in that the catagorical variables have no assumed order or ranking.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
