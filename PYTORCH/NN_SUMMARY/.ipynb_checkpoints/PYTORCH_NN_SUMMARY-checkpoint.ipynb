{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PYTORCH WALKTHROUGH\n",
    "\n",
    "\n",
    "Pytorch is a powerful framework for training neural networks, is produced by facebook and designed to be optimised with GPUs (although not necessary). \n",
    "\n",
    "## Theory - Neural Networks\n",
    "\n",
    "\n",
    "Neural Networks are called such as they are analogs of real biological systems which comprise the learning elements of our brains. In terms of computing, they have some similarities and emulate core features (propogation). The inputs have weights applied to them, with an additiona bias (not shown in diagram) summed up then passed through the activation function.\n",
    "\n",
    "\n",
    "![image](https://upload.wikimedia.org/wikipedia/commons/6/60/ArtificialNeuronModel_english.png)  \n",
    "\n",
    "Mathematically this looks like: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "y &= f(w_1 x_1 + w_2 x_2 + b) \\\\  \n",
    "y &= f\\left(\\sum_i w_i x_i +b \\right)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "With vectors this is the dot/inner product of two vectors:\n",
    "\n",
    "$$\n",
    "h = \\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots  x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_1 \\\\\n",
    "           w_2 \\\\\n",
    "           \\vdots \\\\\n",
    "           w_n\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KEY COMMANDS\n",
    "\n",
    "`torch.sum(features*weights) + bias)`  \n",
    "`torch.exp(-x)`\n",
    "\n",
    "## RESHAPING\n",
    "\n",
    "There are a few options here:   \n",
    "\n",
    "\n",
    "[`weights.reshape()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.reshape)  \n",
    "[`weights.resize_()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.resize_)  \n",
    "[`weights.view()`](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.view).        **# preferred**\n",
    "\n",
    "## MATRIX MULTIPLICATION  \n",
    "\n",
    "`torch.mm()`  **preferred**  \n",
    "`torch.matmul()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(7)                  ### Generate some data\n",
    "features = torch.randn((1, 5))        # Features are 5 random normal numbers\n",
    "weights = torch.randn_like(features)  # Trandn_like = copy shape of features\n",
    "bias = torch.randn((1, 1))            # random normal value "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITHOUT MATRIX MULTIPLICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summing up features, weights and bias into activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate the output of this network using the weights and bias tensors\n",
    "y1 = activation((features*weights).sum() + bias) # sums up two arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TORCH METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = activation(torch.sum(features*weights) + bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n",
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "print(y)\n",
    "print(y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WITH MATRIX MULTIPLICATION\n",
    "\n",
    "Features usually have many rows:\n",
    "Weights usually have many columns: \n",
    "\n",
    "value : **features([[-0.8948, -0.3556,  1.2324,  0.1382, -1.6822]])**   \n",
    "\n",
    "&nbsp;  **weights([[-0.8948],  \n",
    "&nbsp; &nbsp;  &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;  [-0.3556],  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;     [ 1.2324],  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;     [ 0.1382],  \n",
    "&nbsp; &nbsp; &nbsp; &nbsp;  &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp;    [-1.6822]])**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.mm(features,weights) # FAILS DUE TO SHAPE m1: [1 x 5], m2: [1 x 5]\n",
    "weights_tranposed = weights.view(5,1) # Reshape and fix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 1])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights_tranposed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "y = activation(torch.mm(features, weights_tranposed)+ bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FULL TORCH CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of features: torch.Size([1, 5])\n",
      "shape of weights: torch.Size([1, 5])\n",
      "shape of weights is now: torch.Size([5, 1])\n",
      "tensor([[0.1595]])\n"
     ]
    }
   ],
   "source": [
    "# First, import PyTorch\n",
    "import torch\n",
    "def activation(x):\n",
    "    \"\"\" Sigmoid activation function \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        x: torch.Tensor\n",
    "    \"\"\"\n",
    "    return 1/(1+torch.exp(-x))\n",
    "\n",
    "torch.manual_seed(7)                  ### Generate some data\n",
    "features = torch.randn((1, 5))        # Features are 5 random normal numbers\n",
    "weights = torch.randn_like(features)  # Trandn_like = copy shape of features\n",
    "bias = torch.randn((1, 1))            # random normal value \n",
    "\n",
    "print('shape of features: ' + str(features.shape))\n",
    "print('shape of weights: ' + str(weights.shape))\n",
    "\n",
    "weights_tranposed = weights.view(5,1) # Reshape and fix\n",
    "print('shape of weights is now: ' + str(weights_tranposed.shape))\n",
    "y = activation(torch.mm(features, weights_tranposed)+ bias)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STACKING MORE DIMENSIONS\n",
    "\n",
    "The above just took the multiplication of weights on inputs as output and applied activation function.\n",
    "But we can add another set of weights w2 to give a second output, thus having two neurons \n",
    "\n",
    "### SINGLE NEURON \n",
    "\n",
    "` output = [sum(Inputs * weights) + bias]    ----> activation function`\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### ADD MORE NEURONS\n",
    "\n",
    "What has been done so far whilst awesome, is just one neuron, *tip: no of neurons = no of activations* . To increase learning efficiency, rate and power we want to add more neurons. This means expressing our weights as a matrix, and adding more. So we have something (excluding bias and activation)that looks like: \n",
    "\n",
    "` inputs * weights1 = hidden layer `  \n",
    "` hiddenlayer * weights2 = output` \n",
    "\n",
    "The reason we have to express weights as a matrix, is because we have inputs that are **one row** and **three columns**. This means our first weights should be **three rows** and **two columns**. Each column being an output hidden node. Since we now have **2 hidden nodes**, we run the process again with weights that are **two rows** and **one column** the output final node.\n",
    "\n",
    "#### EACH WEIGHT HAS THE ROWS WHICH ARE EQUAL TO THE NUMBER OF INCOMING LAYER\n",
    "\n",
    "### THREE NEURONS  SUDO CODE\n",
    "\n",
    "**All values random normal populated**\n",
    "\n",
    "`inputs_shape = [1,3]`  \n",
    "`w1_shape =  [3,2]`  \n",
    "`hidden_layer = activate[matrixmultiply[inputs,w1] + bias1]`   \n",
    "`w2_shape = [2,1]`  \n",
    "`ouptut = activate[matrixmultiply[h1,w2] + bias2]` \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![md](multilayer_diagram_weights.png)\n",
    "\n",
    "The first layer shown on the bottom here are the inputs, understandably called the **input layer**. The middle layer is called the **hidden layer**, and the final layer (on the right) is the **output layer**. We can express this network mathematically with matrices again and use matrix multiplication to get linear combinations for each unit in one operation. For example, the hidden layer ($h_1$ and $h_2$ here) can be calculated \n",
    "\n",
    "$$\n",
    "\\vec{h} = [h_1 \\, h_2] = \n",
    "\\begin{bmatrix}\n",
    "x_1 \\, x_2 \\cdots \\, x_n\n",
    "\\end{bmatrix}\n",
    "\\cdot \n",
    "\\begin{bmatrix}\n",
    "           w_{11} & w_{12} \\\\\n",
    "           w_{21} &w_{22} \\\\\n",
    "           \\vdots &\\vdots \\\\\n",
    "           w_{n1} &w_{n2}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The output for this small network is found by treating the hidden layer as inputs for the output unit. The network output is expressed simply\n",
    "\n",
    "$$\n",
    "y =  f_2 \\! \\left(\\, f_1 \\! \\left(\\vec{x} \\, \\mathbf{W_1}\\right) \\mathbf{W_2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Features : torch.Size([1, 3])\n",
      "tensor([[-0.1468,  0.7861,  0.9468]])\n",
      "Shape of Weights 1 : torch.Size([3, 2])\n",
      "Shape of weights 2: torch.Size([2, 1])\n",
      "Two hidden layers H1 & H2 are : tensor([[0.6813, 0.4355]])\n",
      "The final value: tensor([[0.3171]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "### Generate some data\n",
    "torch.manual_seed(7) # Set the random seed so things are predictable\n",
    "\n",
    "# Features are 3 random normal variables\n",
    "features = torch.randn((1, 3))\n",
    "no_of_features_columns = features.shape[1]\n",
    "\n",
    "\n",
    "# Define the size of each layer in our network\n",
    "number_inputs = no_of_features_columns     \n",
    "number_hidden = 2                          \n",
    "number_outputs = 1                        \n",
    "\n",
    "# Weights for inputs to hidden layer\n",
    "W1 = torch.randn(number_inputs, number_hidden) # must match number of inputs to desired hidden\n",
    "\n",
    "W2 = torch.randn(number_hidden, number_outputs) # must mat\n",
    "\n",
    "# and bias terms for hidden and output layers\n",
    "B1 = torch.randn((1, number_hidden))\n",
    "B2 = torch.randn((1, number_outputs))\n",
    "\n",
    "print(\"Shape of Features : \" + str(features.shape))\n",
    "print(str(features))\n",
    "print(\"Shape of Weights 1 : \" + str(W1.shape))\n",
    "print(\"Shape of weights 2: \" + str(W2.shape))\n",
    "\n",
    "hidden_layer = activation(torch.mm(features,W1) + B1)\n",
    "output = activation(torch.mm(hidden_layer, W2) + B2)\n",
    "\n",
    "\n",
    "\n",
    "print('Two hidden layers H1 & H2 are : ' + str(hidden_layer))\n",
    "print('The final value: ' + str(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
